%\documentclass{uai2021} % for initial submission
 \documentclass[accepted]{uai2021} % after acceptance, for a revised
                                    % version; also before submission to
                                    % see how the non-anonymous paper
                                    % would look like
% NOTE: Only comment/uncomment the lines above as appropriate,
%       as they will be replaced automatically for papers to be published.
%       Do not make any other change above this note for an accepted
%       version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}

%% Some suggested packages, as needed:
\usepackage{natbib} % has a nice set of citation styles and commands
    \bibliographystyle{plainnat}
    \renewcommand{\bibsection}{\subsubsection*{References}}
\usepackage{mathtools} % amsmath with fixes and additions
% \usepackage{siunitx} % for proper typesetting of numbers and units
\usepackage{booktabs} % commands to create good-looking tables
\usepackage{tikz} % nice language for creating drawings and diagrams

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\md}{\mathrm{d}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newtheorem{thm}{Theorem}
\newtheorem{algo}[thm]{Algorithm}
\newtheorem{ass}[thm]{Assumption}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{defn}[thm]{Definition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{rem}[thm]{Remark}


\newcommand{\pa}{\textnormal{pa}}

\newcommand{\ds}{\text{ds}}
\newcommand{\dt}{\text{dt}}

\newcommand{\disjU}{\mathbin{\dot{\cup}}}

\usepackage{tikz}
\usepgflibrary{arrows}
\usetikzlibrary{shapes}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{positioning}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}


%%%%%%%%%%%%%%%%%%%%%%%%%%%


\title{Equality Constraints and Causal Identification in Linear Hawkes 
Processes}

% The standard author block has changed for UAI 2021 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors in order of decreasing contribution
\author[1]{\href{mailto: Søren Wengel Mogensen 
<swemo@dtu.dk>}{Søren~Wengel~Mogensen}{}} 
% Add affiliations after the authors
\affil[1]{%
    Section of Cognitive Systems\\
    Technical University of Denmark\\
    Denmark
}

\begin{document}
\maketitle

\begin{abstract}
Conditional independence is often used as a testable 
implication of causal models of random variables. In addition, equality 
constraints have been proposed to distinguish between data-generating 
mechanisms. We show that one 
can also find equality constraints in the class of linear Hawkes process, 
extending this theory to continuous-time stochastic processes. We 
show that Hawkes process models in a certain sense satisfy the equality 
constraints 
of a linear structural equation model, and we also show that one may 
find additional constraints. These results
allow more refined constraint-based structure learning in this class of 
processes.
\end{abstract}


\section{Introduction}\label{sec:intro}

In causal inference, the question of what can be learnt about the underlying 
structure from observational data is a classical one 
\citep{spirtes1993,pearl2009}. While most work 
studies models of random variables and without an explicit notion of time, this 
question has also been 
studied for stochastic process models. For a multivariate stochastic 
process, $X_t = (X_t^1, X_t^2, \ldots, X_t^n)$, this is often formalized by 
assuming that the dynamical development of each coordinate process, $X_t^i$,   
depends directly only on a subset of the other coordinate processes which can 
be 
represented by a directed graph which is the \emph{structure} that methods 
should aim to learn. As examples of this, \cite{eichlerHawkes2017, xu2016} learn
graphs describing the structure of multivariate linear Hawkes processes from 
dynamical observation of the process whereas 
\cite{achab2017} use integrated cumulants of 
linear Hawkes processes for structure learning. 
These approaches assume that we have full observation in the sense that every 
coordinate 
process is observed. \cite{meek2014, mogensenUAI2018, mogensenUAI2020} consider 
structure learning based on tests of so-called local independence and provide 
methods 
that can recover structural information based on a marginal distribution only. 

In this paper, we show that one can 
use equality 
constraints in linear Hawkes processes to distinguish between graphical 
structures that 
are indistinguishable using tests of local independence only which enables more 
refined structure learning algorithms. This development 
is analogous to structural causal models in which equality constraints have 
been used for structure learning as and addition to conditional independence 
constraints 
\citep{robins1986,vermaEquiAndSynthesis,robins1999,tian2002,shpitser2014,richardson2017}.

Arguing for the existence of equality constraints also leads us to new 
identification 
results for Hawkes processes, 
and we give a causal interpretation of the parameters which is closely 
related to the so-called cluster representation of a linear Hawkes process.



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hawkes Processes}
\label{sec:hawPro}

Hawkes processes have received much attention in machine learning in recent 
years as a tractable model of mutually exciting streams of events 
\citep{zhou2013b,luo2015,etesami2016,tan2018,xu2018,trouleau2019}. 
The following is a brief introduction to this model class,
see \cite{laubHawkes2015}, \cite{linigerThesis}, and \cite{daleyVere} for 
additional background on Hawkes processes and point processes in general.

We consider a filtered probability space $(\Omega, \mathcal{F}, 
(\mathcal{F}_t), P)$ and an $n$-dimensional point process, $X_t = 
(X_t^1,X_t^2,\ldots,X_t^n)^T$, and let $V= \{1,2\ldots, n\}$. $\mathcal{F}_t$ 
is 
a filtration, that is, a nondecreasing family of $\sigma$-algebras which for 
each time point, $t\in \mathbb{R}$, represents the available information at 
that time. For each 
$\alpha\in 
V$, we say that $X^\alpha$ is a \emph{coordinate process}. Each coordinate 
process is described by a series of \emph{events}, or \emph{points}, that occur 
at time points indexed by $\mathbb{R}$. For $\alpha \in V$, this is formalized 
by random variables, $\{T_i^\alpha\}_{i\in\mathbb{Z}}$, such 
that $T_i^\alpha < T_{i+1}^\alpha$ almost surely. For $s<t$, we let the  
$N_t^\alpha - 
N_s^\alpha$ denote the number of points (events) in the $\alpha$-process 
in the interval $(s,t]$, that is,  $N_t^\alpha - 
N_s^\alpha= \sum_i \mathds{1}_{s \leq T_i^\alpha \leq t}$. We 
assume that no 
two events occur simultaneously. For 
$\alpha\in V$ the \emph{intensity process}, $\lambda_t^\alpha$, 
satisfies

$$
\lambda_t^{\alpha} = \lim_{h \downarrow 0} \dfrac{1}{h} P(N_{t + h}^\alpha  - 
N_t^\alpha = 1
\mid \mathcal{F}_t).
$$

The intensity, $\lambda^\alpha$, therefore describes how likely an 
$\alpha$-event is in the immediate future $(t, t+h)$, for small $h$, given the 
past until time $t$.

\begin{defn}[Linear Hawkes process]
	We say that a point process, $X$, is a \emph{linear Hawkes process} if for 
	all $\beta\in V$ and $t\in \mathbb{R}$ the 
	intensity is of the form
	
	$$
	\lambda_t^\beta = \mu_\beta + \sum_{\beta \in V} \int_{-\infty}^{t} 
	\phi_{\beta\alpha}(t - s) d 
	N_s^\alpha
	$$
	
	\noindent for a constant $\mu_\beta \geq 0$ and nonnegative functions 
	$\phi_{\beta\alpha}$ for $\alpha \in V$.
	\label{def:hawProc}
\end{defn}

We will often write \emph{Hawkes process} instead of \emph{linear Hawkes 
process}. 
We assume throughout for all 
$\alpha$ and $\beta$ that $\phi_{\beta\alpha}$ is a continuous 
function the support of which is contained in $(0, \infty)$. We define the 
$n\times n$ matrix $\Phi$ such that $\Phi_{\beta\alpha} = 
\int_{-\infty}^\infty 
\phi_{\beta\alpha}(s) \ds$. We assume that the spectral radius of $\Phi$ 
(largest 
absolute value of its eigenvalues) is less than 1 which implies that we can 
assume the Hawkes process to have stationary increments and $\lambda_t$ to be a 
stationary process \citep{jovanovic2015, bacry2016}.
We define the $n\times n$ matrix $R=(I_n - \Phi)^{-1}$ where $I_n$ is the 
$n\times n$ identity matrix. $R$ is well-defined due to the assumption on 
the spectral radius of $\Phi$ and furthermore $R = \sum_{i=0}^\infty \Phi^i$. 
For a matrix, $M$, with rows indexed by $I$ and columns indexed by $J$, we let 
$M_{\bar{I}\bar{J}}$ denote the submatrix indexed by $\bar{I}\subseteq I$ and 
$\bar{J}\subseteq J$. We let $dN_t^\alpha$ denote $N_{t+dt}^\alpha - 
N_t^\alpha$ where $dt$ is the 
differential of $t$. 
Following \cite{hawkesJRSSB1971}, for each $\alpha\in V$ we define

$$
\Lambda^\alpha = E(dN_t^\alpha)/dt = E(\lambda_t^\alpha)
$$

\noindent which is independent of $t$ due to stationarity. We let $\Lambda$ 
denote the $n\times n$ diagonal matrix such that $\Lambda_{\alpha\alpha} = 
\Lambda^\alpha$.


\begin{figure}
				\begin{subfigure}{0.48\linewidth}
					\centering
					\includegraphics[scale=.45]{plot1}
				\end{subfigure} \\
				\begin{subfigure}{0.48\linewidth}
					\centering
					\includegraphics[scale=.45]{intPlot}
				\end{subfigure} \\
				\begin{subfigure}{0.48\linewidth}
					\centering
					\includegraphics[scale=.45]{treesPlot}
				\end{subfigure}
				\caption{Top: Example of observation from 5-dimensional Hawkes 
				process ($V = \{1,2,3,4,5\}$. Colors (and vertical placement) 
				indicate coordinate 
				process. For each point 
				(circle in the plot) time of occurrence is known as well as 
				which coordinate process the event occurs in. Middle: intensity 
				processes, $\lambda_t^\alpha, \alpha\in V$, corresponding to 
				event 
				histories from the top plot (for simplicity only displayed 
				events 
				count in the intensities). The intensities are stochastic 
				processes and from the stochastic integral in Definition 
				\ref{def:hawProc} one can see that an $\alpha$-event increases 
				the $\beta$-intensity if $\phi_{\beta\alpha} \neq 0$. 
				Bottom: same event plot showing the 
				clusters of the observed process. Vertical placement indicates 
				which coordinate process the events occurred in. Numbers 
				indicate 
				generation 
				of the event and line segments indicate ancestry. For instance, 
				the 1-2-3-4 cluster was produced from a generation 0-event of 
				the 
				$1$-type which produced an event of the $2$-type and so forth.}
			\label{fig:haw}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Integrated Covariance}

We will define two probabilistic concepts that describe the covariance between 
coordinate proceses. One 
is seen to be an integrated (and time-independent) measure of covariance 
(below) and 
one is seen to be a 
time-dependent measure of covariance (see Subsection \ref{ssec:idCauFunc}). We 
define an $n\times n$ matrix, $C$, such that for $\alpha,\beta\in V$,

\begin{align}
	C_{\beta\alpha} dt = \int_{-\infty}^{\infty} E(dN_t^\beta 
	dN_{t+\tau}^\alpha) - E(dN_t^\beta )(dN_{t+\tau}^\alpha) d\tau,
\end{align}

which is seen to contain the \emph{integrated covariances} of each pair of 
coordinate processes. The following matrix equation holds 
\citep{jovanovic2015,	 
achab2017}.


\begin{align}
	C = R \Lambda R^T = (I_n - \Phi)^{-1}\Lambda(I_n - \Phi)^{-T},
	\label{eq:covarEq}
\end{align}


where $-T$ denotes transposing and 
inverting a matrix.

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Cluster Representation}

In Definition \ref{def:hawProc}, Hawkes processes are introduced by specifying 
the intensity processes. One can also define them by using a so-called cluster 
representation which, as we will see, lends itself to a straight-forward causal 
interpretation.

For each $\alpha\in V$, \emph{generation 0}-events occur as points a 
homogeneous Poisson process with rate $\mu_\alpha$. Each generation 0-event 
creates a \emph{Hawkes tree}, that is, a cluster of events (the clusters are 
independent). The clusters are generated recursively. Each event of type 
$\alpha$ at time $s$ in a cluster have \emph{child} events of type $\beta$ 
according 
to an inhomogeneous Poisson process with rate $\phi_{\beta\alpha}(t-s)$ for $t 
> s$, and these processes are all independent. This exemplified in Figure 
\ref{fig:haw} (bottom) in which four 
generation 0-events are shown, each of them starting a cluster. Parent-child 
relations are shown using line segments. Note that 
$\phi_{\beta\alpha} = 0$ means that there will be no $\beta$-children from an 
$\alpha$-event, though there could be $\beta$-descendants further down from the 
$\alpha$-event if 
there exists $\gamma$ such that $\phi_{\gamma\alpha} \neq 0$ and 
$\phi_{\beta\gamma} \neq 0$. The observed Hawkes process is then the 
superposition of the events in all clusters. One should note that when 
observing a Hawkes process, we only assume access to 
$(\alpha_i, t_i)$, that is, a set of event times and types, not the clusters.

The parameters in $\Phi$ and $R$ have straightforward interpretations. From the 
cluster process definition of the Hawkes process, it follows that 
$\Phi_{\beta\alpha}$ is the 
expected number of child events of type $\beta$ from an 
$\alpha$-event. More generally, for $i = 0,1,2,\ldots$, 
$(\Phi^i)_{\beta\alpha}$ is the 
expected number of $\beta$-descendants in the $i$'th generation of a Hawkes 
tree rooted at an 
$\alpha$-event. This implies that $R_{\beta\alpha}$ is the expected total 
number of $\beta$-descendants on such a tree
\citep{jovanovic2015}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Causal Interpretation}

In this section, we will define what we mean by a {\it causal} Hawkes process. 
\cite{mogensenUAI2020} defined a causal Hawkes process by requiring that the 
set 
of functions $\{\phi_{\beta\alpha}\}$ also describes how events enter the 
intensities in intervened systems where the trajectory of some 
coordinate processes is determined exogenously. We will instead use the cluster 
representation to give a causal
interpretation based on a type of \emph{injection intervention}. That is, the 
observational state of the system is given by the Hawkes process as defined 
above. An interventional state is defined by a set of \emph{injections}, 
$\{(\alpha_i, t_i) \}_{i =1,\ldots, M} $, such that each injection is an 
element in $V\times \mathbb{R}$. Each injection is 
a root in a Hawkes tree and the interventional processes are given as the 
superposition of all intrinsic and interventional trees. We then say that a 
linear Hawkes process is {\it 
causal} if a Hawkes tree rooted at an injection event has the same distribution 
as an 
intrinsic, or non-injection, Hawkes tree for all coordinate processes $\alpha 
\in V$. In 
other words, the intervention events 
propagate in the same way through the system as the intrinsic events. This 
causal assumption is seen to correspond closely to 
the assumption of \emph{autonomy} or \emph{modularity} which is often used to 
define causal models 
\citep{pearl2009, petersElements2017}.

We note that this type of interventions is a simplistic version of 
interventions that are undertaken in some fields of 
science, e.g., in neuroscience where experimenters may intervene on neurons by 
means of extracellular electrical stimulation. This, however, is not expected 
to break their intrinsic dynamics nor 
sever causal mechanisms \citep{meffin2012, komarov2019}. It is also similar to 
how interventions are 
often 
modelled in linear control theory in which they are external forces (possibly 
dependent on the past of the process) that change the dynamics of the system 
without necessarily breaking any connections \citep{aastrom2008, zabczyk2020}.

\begin{defn}[Causal effects]
	We will say that $\Phi_{\beta\alpha}$ is the \emph{direct (causal) effect} 
	from 
	$\alpha$ on $\beta$ and that $R_{\beta\alpha}$ is the \emph{total (causal) 
	effect} from $\alpha$ on $\beta$. We will say that $\phi_{\beta\alpha}$ is 
	the 
	{\it (causal) link function} from $\alpha$ to $\beta$.
	\label{def:cauEff}
\end{defn}

The interpretation of the entries in $\Phi$ and $R$ and the causal assumption 
justify the above definitions. If 
we inject an 
interventional event of type $\alpha$ at time $s \in \mathbb{R}$, this 
event on average has $\Phi_{\beta\alpha}$ events 
of type $\beta$ as first-generation (or \emph{direct}) descendants (when 
considering all $t > s$). The Hawkes tree rooted at the 
interventional $\alpha$-event on average has a total of $R_{\beta\alpha}$ 
events of 
type $\beta$, counting the injectional event if $\alpha=\beta$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Graphical representation}

A {\it graph} is a pair $(V,E)$ where $V$ is a finite set of nodes and $E$ is a 
set edges, each between a pair of (not necessarily distinct) nodes. We will  
consider {\it directed mixed graphs} (DMGs), i.e., graphs such that every edge 
is 
either \emph{directed}, $\rightarrow$, or \emph{bidirected}, $\leftrightarrow$. 
We use $\sim$ to denote a generic edge of either type. If the edge 
$\alpha\rightarrow\beta$ is in the graph $\mathcal{G}$, we say that $\alpha$ is 
a \emph{parent} of $\beta$, and we let $\pa_\mathcal{G}(\beta)$ denote the set 
of parents 
of $\beta$ in the graph $\mathcal{G}$.

Given a causal Hawkes process with coordinate processes indexed by $V = 
\{1,2,\ldots,n\}$, we construct its \emph{causal graph}, $\mathcal{D} = (V,E)$, 
such that 
$\alpha \rightarrow_\mathcal{D} \beta$ if and only if $\phi_{\beta\alpha} \neq 
0$. 
Due 
to the assumptions of continuous and nonnegative $\phi_{\beta\alpha}$, we see 
that if $\alpha\rightarrow\beta$ in the causal graph, then there exist a 
nontrivial interval such 
that $\phi_{\beta\alpha} > 0$ on this interval.

A {\it walk between $\alpha$ and $\beta$} is an ordered, alternating sequence 
of nodes, $\gamma_i$, and edges, $\sim_j$,

$$
\alpha = \gamma_0 \sim_1 \gamma_1 \sim_2 \ldots \gamma_{m-1} \sim_m \gamma_{m} 
= \beta
$$

such that for each $i = 1,\ldots,m$, the edge $\sim_i$ is between 
$\gamma_{i-1}$ and $\gamma_i$. We say that $\alpha$ and $\beta$ are 
\emph{endpoint nodes}. A {\it path} is a walk such that no node occurs 
more than once. We say that a path from $\alpha$ to $\beta$ is \emph{directed} 
if every edge is directed and points towards $\beta$, and a \emph{directed 
cycle} is a directed path from $\alpha$ to $\beta$ composed with the edge 
$\beta\rightarrow\alpha$. We say that a DMG is \emph{acyclic} if it contains no 
\emph{directed cycles}. We say that a walk $\alpha \sim \gamma_1 \sim \ldots 
\sim 
\gamma_k \sim \beta$ is \emph{unobserved} if $\gamma_i \in U$ for all $i = 
1,\ldots, k$ and $k \geq 1$. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Representation as a Linear SEM}

The relation between the observed quantity, $C$, and the parameters in $\Phi$ 
in 
Equation (\ref{eq:covarEq}) is seen to be very similar to the relation between 
causal parameters and the covariance matrix of a \emph{linear structural 
	equation model} (linear SEM) and we will leverage this fact. 
There is 
a number of papers with 
identification results in this model class \citep{brito2002, tian2007, 
	tian2009, 
	foygelHalftrek2012, 
	chenNIPS2016, drton2016, 
	weihs2018}, though some results only apply to \emph{acyclic} linear SEMs 
	(see a 
definition of acyclicity below). \cite{bollen1989} provides an introduction to 
the model class. We give a short introduction here to describe the connection 
to 
Hawkes processes. Let $X = (X_1,\ldots,X_n)^T$ be a vector of random variables 
and 
$\varepsilon = 
(\varepsilon_1,\ldots,\varepsilon_n)^T$ is a vector of zero-mean noise terms,

\begin{align}
X = BX + \varepsilon
\label{eq:SEM}
\end{align}

such that the matrix $B$ has zeros on the diagonal. We let $\Sigma_\varepsilon$ 
denote the covariance of $\varepsilon$. We assume that the spectral radius of 
$B$ is less than one, and the
covariance matrix of $X$ is then

\begin{align}
C_X = (I_n - B)^{-1}\Sigma_\varepsilon (I_n - B)^{-T},
\label{eq:covSEM}
\end{align}

see \cite{hyttinen2012}. Often one will assume that some entries of $B$ and 
$\Sigma_\varepsilon$ are zero which can be encoded by a DMG (also known as a 
\emph{mixed graph}) such that if 
$\alpha \rightarrow \beta$ is not in the DMG, then $B_{\beta\alpha} = 0$ and 
if, 
$\alpha\rightarrow\beta$ is not in the DMG, then 
$(\Sigma_\varepsilon)_{\beta\alpha} = (\Sigma_\varepsilon)_{\alpha\beta} = 0$. 
The 
error terms are often assumed to be Gaussian. We note that acyclicity is 
sometimes assumed, 
i.e., that the DMG corresponding to the model is 
acyclic, though we are not making 
this assumption.

We should note that in the linear SEM literature, some authors parametrize the 
model using a 
tranposition of $B$, $X = B^TX + \varepsilon$, though the notation we use 
aligns better with the common 
notation 
in Hawkes process literature.

One see immediately the similarity between Equations (\ref{eq:covarEq}) 
and (\ref{eq:covSEM}) in that the observable left-hand side of the equations in 
both cases impose the same restrictions on the model parameters, and we use 
this similarity to translate results to the Hawkes models. There are two 
questions that should be addressed. First, the diagonal of $B$ in 
the linear SEM is often assumed to be zero, and we will handle this by 
re-writing 
Equation (\ref{eq:covarEq}) slightly (\ref{sssec:normEff}). Second, $\Lambda$ 
is diagonal in Equation (\ref{eq:covarEq}) whereas there is no such restriction 
a priori on $\Sigma_\varepsilon$ and we will see that this discrepancy 
corresponds to marginalization of the systems (\ref{sssec:marg}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Marginalization}
\label{sssec:marg} 

We will mostly be interested in systems that are only partially observed in 
which case only a proper submatrix of $C$ in Equation (\ref{eq:covarEq}) is 
observed. We let $V = O \disjU 
U$ and think of $O$ as the set of observed processes. Let $\bar{C} = C_{OO}$ be 
the observed submatrix of $C$. We 
have 
that 

\begin{align}
\bar{C} = (I - \bar{\Phi})^{-1}\Theta(I - \bar{\Phi})^{-T}
\end{align}

where 

\begin{align*}
\bar{\Phi} & = {\Phi}_{OO} +  {\Phi}_{OU}(I_l - 
{\Phi}_{UU})^{-1}{\Phi}_{UO} \\
\Theta & = (I_k-\bar{\Phi})[(I_n - {\Phi})^{-1}{\Lambda}(I - 
{\Phi})^{-T}]_{OO}(I_k-\bar{\Phi})^T. \\
\end{align*}

The assumption that $\Phi$ has spectral radius less than one does not imply 
that $\bar{Phi}$ does as well, though $I - \bar{\Phi}$ is guaranteed to be 
invertible \cite{hyttinen2012}. Let 
$\tilde{\Lambda} 
= D^{-1}\Lambda D^{-1}$. Using Schur complements, we can write (see the 
appendix)

\begin{align*}
\Theta =  {\Lambda}_{OO} + & {\Phi}_{OU}((I_n - 
{\Phi})^{-1})_{UU}{\Lambda}_{UU} \times  \\ & ((I_n - 
{\Phi})^{-T})_{UU}
{\Phi}_{OU}^T.
\end{align*}

$\Theta$ is clearly positive semidefinite, and positive definite if 
$\Lambda$ is. The above computations are of course the same as in the case of a 
linear 
structural equation model. We will use \emph{latent projections} to find 
graphical representations of partially observed Hawkes processes.

\begin{defn}[Latent projection, \cite{vermaEquiAndSynthesis,richardson2017}]
	\label{def:latProj}
	Let $\mathcal{G} = (V,E)$ be a DMG. Its latent projection on nodes $O$ is 
	the DMG $\bar{\mathcal{G}} = (O,F)$ such that for $\alpha,\beta\in O$ we 
	have $\alpha\rightarrow\beta$ in $\bar{\mathcal{G}}$ if there is an 
	directed walk from $\alpha$ to $\beta$ such that every non-endpoint node is 
	in $V\setminus O$. We have 
	$\alpha\leftrightarrow\beta$ in $\bar{\mathcal{G}}$ if there is a walk 
	between $\alpha$ and $\beta$ with heads at 
	both $\alpha$ and $\beta$ such that every non-endpoint node is a 
	noncollider and is in 
	$V\setminus O$.
\end{defn}

Note that for a walk in the above definition the set of non-endpoint nodes can 
be empty and therefore if $\alpha\rightarrow\beta$ is in $\mathcal{G}$, then it 
is 
also in its latent projection whenever $\alpha,\beta\in O$. We also note that 
the definition uses walks to 
allow for cycles \citep{mogensen2018}. In DAG-based models, this is 
not needed 
and the definition is often stated using paths 
\citep{richardson2017}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Normalized Effects}
\label{sssec:normEff}

When $\Phi$ is a matrix of direct effects of a Hawkes process (or marginalized 
effects, $\bar{\Phi}$), the diagonal 
elements are not necessarily zero. On the other hand, in a linear SEM the 
diagonal of $B$ 
(Equation (\ref{eq:SEM})) is commonly assumed to be zero. We introduce 
the matrix $\tilde{\Phi}$ by scaling each column of $\bar{\Phi}$ by its 
diagonal element and placing zeros on the diagonal,

$$
\tilde{\Phi}_{\beta\alpha} = 
\bar{\Phi}_{\beta\alpha}/(1-\bar{\Phi}_{\beta\beta})  \text{ 
	if } \beta\neq \alpha, \text{ and } 
\tilde{\Phi}_{\beta\alpha} 
= 0 \text{ if } \beta= \alpha,
$$

where $I-\tilde{\Phi}$ is invertible \cite{hyttinen2012}. \cite{hyttinen2012} 
study linear SEMs and use the same approach to derive 
so-called canonical linear 
cyclic models from models with self-loops. We think of the above operation as a 
type of normalization, and if this is done in the full model (no 
marginalization), we 
will say that the normalized parameters, that is, 
${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta}) $,
are \emph{normalized direct
	(causal) effects}. It seen is that a normalized direct effect, 
${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta}) $, $\alpha\neq\beta$, is the 
average number of 
direct 
descendants of type 
$\beta$ from an $\alpha$-event, counting every direct self-triggered event in 
the $\beta$-process. 
That is, on the Hawkes tree rooted at $\alpha$, 
${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta}) $ is 
the expected number of $\beta$-events on subtrees of the form $\alpha - \beta - 
\beta - \ldots - 
\beta$ for any number of $\beta$-events. 
Let $D$ denote a 
diagonal matrix such that the $\alpha$'th diagonal element equals 
$1 - \bar{\Phi}_{\alpha\alpha}$. Then


\begin{align}
\bar{C} & = (I - \bar{\Phi})^{-1}\Theta(I - \bar{\Phi})^{-T} \nonumber  \\
& = (D(I - \tilde{\Phi}))^{-1}\Theta (D(I - \tilde{\Phi}))^{-T} \nonumber \\
& = (I - \tilde{\Phi})^{-1}D^{-1}\Theta D^{-1}(I - 
\tilde{\Phi})^{-T}.
\label{eq:Gtilde}
\end{align}

If we consider a Hawkes process and its causal graph, $\mathcal{D} = (V,E)$, we 
see that the latent projection of $\mathcal{D}$, $\mathcal{G} = (O,F)$, 
$O\subseteq V$, represents the observed covariance, $\bar{C} = C_{OO}$ in the 
following way. If $\alpha\rightarrow\beta$ is not in $\mathcal{G}$, then 
$\bar{\Phi}_{\beta\alpha}=0$. If $\alpha\leftrightarrow\beta$ is not in 
$\mathcal{G}$, then $\Theta_{\beta\alpha} = \Theta_{\alpha\beta} = 0$. Omitting 
self-loops, this corresponds to the graphical representation of a linear SEM. 
This means that the observable integrated covariances after rewriting are 
parametrized exactly like a linear SEM and we will use this to obtain 
identification results (Section \ref{sec:id}) and equality constraints (Section 
\ref{sec:eq}).

Note that while 
this 
casts the Hawkes integrated covariance as a set of equations describing the 
covariance 
matrix of a linear SEM, the Hawkes parameters are required to be nonnegative. 
	

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Identification}
\label{sec:id}

Using constraints on the causal graph, we will discuss the identification of 
the entries in the 
matrix $\Phi$ and the functions $\phi_{\beta\alpha}$. To give a formal 
definition 
of 
identification, consider two stationary Hawkes 
processes, $M_1\in\mathbb{M}$ and $M_2\in\mathbb{M}$, with coordinate processes 
$V_1$ and $V_2$, 
respectively, for a class of models $\mathbb{M}$. Let $ \tilde{P}_1$ and $ 
\tilde{P}_2$ be their marginal 
distributions over a common observed set of 
coordinate processes, $O$, let $f$ be a function of 
such a marginal distribution, and let 
$\alpha,\beta\in O$. Finally, let $\Gamma_{\beta\alpha}^i$ be a parameter and 
$\phi_{\beta\alpha}^i$ the link function from $\alpha$ to 
$\beta$ in $M_i$, respectively ($\Gamma_{\beta\alpha} = \Phi_{\beta\alpha}, 
\Gamma_{\beta\alpha}=\tilde{\Phi}_{\beta\alpha},$ or $\Gamma_{\beta\alpha} = 
\Phi_{\beta\alpha}/(1-\Phi_{\beta\beta})$, e.g.). We will say that the 
parameter 
$A$ \emph{is 
identified from} 
$f(\tilde{P})$ in the class $\mathbb{M}$ if $f(\tilde{P}_1) = f(\tilde{P}_2)$ 
implies $\Gamma_{\beta\alpha}^1 = 
\Gamma_{\beta\alpha}^2$. Similarly, we say 
$\phi_{\beta\alpha}$ is identified from $f(P)$ in the class $\mathbb{M}$ if 
$f(\tilde{P}_1) = 
f(\tilde{P}_2)$ 
implies $\phi_{\beta\alpha}^1 = \phi_{\beta\alpha}^2$. Note that while 
identification 
is often only discussed from the 
observed distribution in its entirety, we use $f$ to distinguish simpler 
statistics which can be sufficient for identification. As we will see some 
parameters are identified from the integrated covariance matrix in which case 
the inferential problem is reduced to a finite-dimensional one.

%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Identification as a Linear SEM}

The idea is now 
that one can use any available identification result from the (cyclic) linear 
structural equation toolbox to show identification of the entries in 
$\tilde{\Phi}$ such as those provided by, e.g., \cite{foygelHalftrek2012, 
	chenNIPS2016, 
	weihs2018}. 
These are often 
formulated in terms of graphical 
conditions, and we note that the 
marginalized set of equations correspond to a linear structural 
equation model described by the latent projection of the Hawkes causal graph 
(Definition \ref{def:latProj}). We also note that one typically has to 
distinguish between \emph{global} and \emph{generic} identification of 
parameters.

\begin{prop}
	Let $\alpha,\beta \in O$, $\alpha\neq \beta$. The parameter 
	${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta}) $ is 
	identified 
	from the observed integrated covariance if there 
	are no unobserved directed paths from 
	$\alpha$ to $\beta$ and $\tilde{\Phi}_{\beta\alpha}$ is identified from 
	$\bar{C}$.
\end{prop}

\begin{proof}
	Note that ${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta}) = 
	\tilde{\Phi}_{\beta\alpha}$ as there 
	are no 
	unobserved directed paths from $\alpha$ to $\beta$.
\end{proof}

\begin{prop}
	Let $\alpha\in O$. The parameter $\Phi_{\alpha\alpha}$ is identified from 
	the 
	integrated mean and covariance if all 
	parents of $\alpha$ are observed and $\Theta_{\alpha\alpha}$ is identified 
	from 
	$\bar{C}$.
\end{prop}

\begin{proof}
	If all parents of $\alpha$ are observed, then $\Theta_{\alpha\alpha} = 
	\tilde{\Lambda}_{\alpha\alpha} = \Lambda_{\alpha\alpha}/(1 - 
	\bar{\Phi}_{\alpha\alpha})^2 = \Lambda_{\alpha\alpha}/(1 - 
	\Phi_{\alpha\alpha})^2$.
\end{proof}

If the spectral radius of $\tilde{\Phi}$ is less than one, all parents of 
$\alpha$ are observed and $\tilde{\Phi}_{\alpha\gamma}$ is 
identified for 
 every 
 $\gamma\in V$, then $\Theta_{\alpha\alpha}$ is identified from $\bar{C}$.
This follows from the $(\alpha,\alpha)$-equation in the matrix equation in 
(\ref{eq:expandTheta}).

It is immediate that $\Phi_{\beta\alpha}$ is identified if 
${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta})$ and $\Phi_{\alpha\alpha}$ are 
identified. Above 
the 
conditions ensure that the parameters of the marginal system equals that of the 
original system and therefore identification in the marginal system implies 
identification in the normalized system. One could of course also consider 
identification of parameters that do not occur in the original example and also 
interpret these, e.g., parameters in $\tilde{\Phi}$ that 
correspond to aggregating effects along multiple unobserved paths, similarly to 
the 
interpretation of the normalized parameters, 
${\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta})$.


\begin{exmp}
			\begin{figure*}
				%\begin{tabular}{cc}
				\begin{subfigure}{0.3\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.7]
					\tikzset{vertex/.style = {shape=circle,draw,minimum 
					size=1.5em, 
							inner 
							sep = 0pt}}
					\tikzset{edge/.style = {->,> = latex', thick}}
					\tikzset{edgebi/.style = {<->,> = latex', thick}}
					\tikzset{every loop/.style={min distance=8mm, looseness=5}}
					\tikzset{vertexFac/.style = {shape=rectangle,draw,minimum 
							size=1.5em, 
							inner sep = 0pt}}
					
					% vertices
					%\draw [line width=35pt,opacity=0.1, blue,line 
					%cap=round,rounded
					%corners] (0,0.5) -- (0,2) -- (-1.5,1.5) -- (0,0.5);
					\node[vertex] (a) at  (-4,0) {$1$};
					\node[vertex] (b) at  (-2,0) {$2$};
					\node[vertex] (c) at  (0,0) {$3$};
					\node[vertex] (d) at  (2,0) {$4$};
					\node[vertex] (e) at  (0,1.25) {$5$};
					
					\node at (-3.5,1.75) {\Large \textbf{A}};
					
					%edges
					
					\draw[edge] (a) to (b);
					\draw[edge] (b) to (c);
					\draw[edge] (c) to (d);
					\draw[edge, bend left = 30] (d) to (b);
					\draw[edge] (e) to (d);
					\draw[edge] (e) to (b);
					\draw[edge, bend right = 35, opacity = 0] (a) to (d);
					
					\end{tikzpicture}
				\end{subfigure}\hspace{.05\linewidth}%
				\begin{subfigure}{0.3\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.7]
					\tikzset{vertex/.style = {shape=circle,draw,minimum 
					size=1.5em, 
							inner 
							sep = 0pt}}
					\tikzset{edge/.style = {->,> = latex', thick}}
					\tikzset{edgebi/.style = {<->,> = latex', thick}}
					\tikzset{every loop/.style={min distance=8mm, looseness=5}}
					\tikzset{vertexFac/.style = {shape=rectangle,draw,minimum 
							size=1.5em, 
							inner sep = 0pt}}
					
					% vertices
					%\draw [line width=35pt,opacity=0.1, blue,line 
					%cap=round,rounded
					%corners] (0,0.5) -- (0,2) -- (-1.5,1.5) -- (0,0.5);
					\node[vertex] (a) at  (-4,0) {$1$};
					\node[vertex] (b) at  (-2,0) {$2$};
					\node[vertex] (c) at  (0,0) {$3$};
					\node[vertex] (d) at  (2,0) {$4$};
					\node[vertex, opacity = 0] (e) at  (0,1.25) {};
					
					\node at (-3.5,1.75) {\Large \textbf{B}};
					
					%edges
					
					\draw[edge] (a) to (b);
					\draw[edge] (b) to (c);
					\draw[edge] (c) to (d);
					\draw[edge, bend left = 20] (d) to (b);
					%\draw[edge] (e) to (d);
					%\draw[edge] (e) to (b);
					\draw[edge, bend right = 35, opacity = 0] (a) to (d);
					\draw[edge, bend left = 35, <->] (b) to (d);
					
					\end{tikzpicture}
				\end{subfigure}\hspace{.05\linewidth}%
				\begin{subfigure}{0.3\linewidth}
					\centering
					\begin{tikzpicture}[scale=0.7]
					\tikzset{vertex/.style = {shape=circle,draw,minimum 
					size=1.5em, 
							inner 
							sep = 0pt}}
					\tikzset{edge/.style = {->,> = latex', thick}}
					\tikzset{edgebi/.style = {<->,> = latex', thick}}
					\tikzset{every loop/.style={min distance=8mm, looseness=5}}
					\tikzset{vertexFac/.style = {shape=rectangle,draw,minimum 
							size=1.5em, 
							inner sep = 0pt}}
					
					% vertices
					%\draw [line width=35pt,opacity=0.1, blue,line 
					%cap=round,rounded
					%corners] (0,0.5) -- (0,2) -- (-1.5,1.5) -- (0,0.5);
					\node[vertex] (a) at  (-4,0) {$1$};
					\node[vertex] (b) at  (-2,0) {$2$};
					\node[vertex] (c) at  (0,0) {$3$};
					\node[vertex] (d) at  (2,0) {$4$};
					\node[vertex, opacity = 0] (e) at  (0,1.25) {};
					
					\node at (-3.5,1.75) {\Large \textbf{C}};
					
					%edges
					
					\draw[edge] (a) to (b);
					\draw[edge] (b) to (c);
					\draw[edge] (c) to (d);
					\draw[edge, bend left = 20] (d) to (b);
					%\draw[edge] (e) to (d);
					%\draw[edge] (e) to (b);
					\draw[edge, bend right = 35] (a) to (d);
					\draw[edge, bend left = 35, <->] (b) to (d);
					
					\end{tikzpicture}
				\end{subfigure}
				%\end{tabular}
				\caption{\label{fig:smallCycle} \textbf{A}: Directed graph such 
					that 
					each node represents a coordinate process and an edge 
					$\alpha 
					\rightarrow \beta$ implies that $\Phi_{\beta\alpha}\neq 0$. 
					\textbf{B}: Latent projection of the graph \textbf{A} on 
					nodes $O = 
					\{1,2,3,4\}$. \textbf{C}: Graphs \textbf{B} and \textbf{C} 
					are 
					Markov 
					equivalent (entail the same $\mu$-separations), yet are 
					distinguishable using an equality constraint on the 
					observable 
					matrix $\bar{C} = C_{OO}$. Loops 
					(self-edges) are omitted from 
					graphs \textbf{A}, \textbf{B}, and \textbf{C}.}
			\end{figure*}
			
	Consider the graph in Figure \ref{fig:smallCycle}\textbf{A} and assume that 
	this is the causal graph of a Hawkes process on nodes $V = \{1,2,3,4,5\}$ 
	such that coordinate process $5$ is unobserved. If we apply the 
	identification criterion of \cite{weihs2018}, we find that every 
	$\tilde{\Phi}_{\beta\alpha}$ 
	such that 
	$\alpha,\beta\in O$ is identified from $\bar{C}$ (note that 
	$\tilde{\Phi}_{\beta\alpha}={\Phi}_{\beta\alpha}/(1-{\Phi}_{\beta\beta})$ 
	for $\alpha,\beta\in O$ in this example). It 
	follows 
	from the 
	above propositions that $\Phi_{11}, \Phi_{23}$, and $\Phi_{33}$ are 
	identified from 
	the integrated mean and covariance.
	
	On the other hand, one can show by direct computation that $\Phi_{21}, 
	\Phi_{22}, 
	\Phi_{24}, 
	\Phi_{43},$ and $\Phi_{4,4}$ are not identified. If we consider 
	$\Phi_{21}$, we can 
	understand the lack of identication from the fact that $\Phi_{22}$ 
	cannot 
	be identified. 
	Intuitively, this is explained by its unobserved parent process which 
	implies that we 
	cannot, using only the integrated covariance, separate the contributions 
	from coordinate processes 2 and 5 to the observed covariance. 
\end{exmp}


%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Identifying Link Functions}
\label{ssec:idCauFunc}

It is only natural that we can obtain stronger identification results when we 
use more aspects of the distribution. Following 
\cite{bacry2016}, we define the (non-singular part of) the \emph{infinitesimal 
covariance},

\begin{align}
	v_{\beta\alpha}(t' - t) = E(dN_t^\beta dN_{t'}^\alpha) & - 
	\Lambda^\beta\Lambda^\alpha dt dt' \nonumber \\ & - \Lambda^\beta 
	\varepsilon_{\beta\alpha}
	\delta(t'-t) dt,
	\label{eq:infv}
\end{align}

where $\delta$ is a Dirac delta function and $\varepsilon_{\beta\alpha} = 
\mathds{1}_{(\beta=\alpha)}$, and

$$
g(t) = v^T(t)\Lambda^{-1}.
$$

We let $\Phi(t)$ denote the matrix of functions such that 
$\Phi(t)_{\beta\alpha} = 
\phi_{\beta\alpha}(t)$. \cite{hawkesJRSSB1971} gave the following $n\times n$ 
equality,

\begin{align}
	g(t) = \Phi(t) + \Phi(t) \star g(t),
	\label{eq:covCauFunc}
\end{align}

where $\star$ means that $(\Phi(t) \star g(t))_{\beta\alpha} = \sum_{\gamma \in 
V} 
\int_{-\infty}^{\infty} \phi_{\beta\gamma}(t - s) g_{\gamma\alpha}(s) \md s$. 
\cite{bacry2016} 
prove that if we consider $\Phi(t)$ 
the 
unknown in the above equation, there is a unique solution (such that 
$\phi_{\beta\alpha}$ is in $L^1$ and its support is in $(0,\infty)$ for all 
$\alpha,\beta\in V$).

\begin{prop}
	Let $\mathcal{D} = (O\disjU U, E)$. If $\pa_\mathcal{D}(\beta) \subseteq 
	O$, then for all $\alpha$ the function $\phi_{\beta\alpha}$ is identified 
	from 
	the infinitesimal covariance process of the $O$-processes and 
	$\Lambda_{OO}$.
	\label{prop:gPaId}
\end{prop}

\begin{proof}
	In Equation \ref{eq:covCauFunc}, we note that $\phi_{\beta\alpha}$ only 
	appears in the $\beta$-row of the equations. Every parent of $\beta$ is 
	observed and therefore this row only depends on observed quantities. If 
	there were two solutions to the equations in the $\beta$-row such that 
	$\phi_{\beta\alpha}$ differed between the two solutions, then there would 
	also 
	be two solutions to Equation (\ref{eq:covCauFunc}) which is a contradiction.
\end{proof}

Under the conditions of Proposition \ref{prop:gPaId}, it also follows 
immediately that $\Phi_{\beta\alpha}$ is identified from the observed 
infinitesimal covariance process and $\Lambda_{OO}$ we will use this in Section 
\ref{ssec:eqInfCov} to obtain further constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Equality Constraints}
\label{sec:eq}

It is well-known that partially observed DAG-models satisfy 
constraints that 
are not described by conditional independence some of which are known as 
\emph{equality constraints}. \cite{richardson2017} treat the general (acyclic) 
case 
while \cite{chen2014, chenNIPS2016} consider equality constraints in linear 
SEMs. In DAG-models equality constraints are testable implications that may 
bring strictly more information about the underlying structure than using 
conditional independence alone. We will see that the analogous statement holds 
for the Hawkes process models, though the relevant notion of independence to 
compare with is 
that of \emph{local independence}. Local independence is an asymmetric notion 
of independence which can be defined 
for stochastic processes \citep{schweder1970, aalen1987, didelez2000, 
didelez2008}. The following definition is specific to point processes, however, 
it can be extended to other classes of stochastic processes 
\citep{aalen1987,didelez2006,mogensenUAI2018,mogensen2018}.

\begin{defn}[Local independence]
	Let $X = (X^1,\ldots,X^n)^T$ be a multivariate point process with intensity 
	processes $\lambda$ and let $V=\{1,\ldots,n\}$. Let $A,B,C,D \subseteq V$. 
	Let $\mathcal{F}_t^D$ denote the natural filtration of $X^D$, i.e., the 
	filtration generated by the coordinate processes in $D$. We say that $X^B$ 
	is \emph{locally independent of $X^A$ given $X^C$} if for all $\beta\in B$ 
	and 
	for all $t\in \mathbb{R}$,
	
	$$
	E(\lambda_t^\beta \mid \mathcal{F}_t^{A\cup C})
	$$
	
	is adapted to the filtration $\mathcal{F}_t^C$.
	\label{def:li}
\end{defn}

Local independence has been used for structure learning in stochastic process 
models 
\citep{meek2014, mogensenUAI2018, thams2019, mogensenUAI2020}, analogously to 
how conditional independence is used for 
constraint-based structure learning in classical models \citep{spirtes1993, 
spirtesSearchChapHandbook}. Structure learning based on local independence 
typically assumes 
\emph{faithfulness}, that is, that $X^B$ is locally independent of $X^A$ given 
$X^C$ if and only if $B$ is $\mu$-separated from $A$ given $C$ in 
$\mathcal{G}$, the DMG 
representing the model
($\mu$-separation, or $\delta$-separation, is a concept analogous to $m$-, or 
$d$-separation \citep{didelez2000, didelez2008, mogensen2018}). The 
\emph{Markov equivalence class} of $\mathcal{G}$ is the set of DMGs that encode 
exactly the same set of $\mu$-separations, 
hence local independences. An obvious 
question is therefore whether there are constraints imposed by the 
 structure that are not described by local independence, or $\mu$-separation. 
 This is indeed the 
 case 
 and such constraints can be used to distinguish between graphical structures 
 that imply the same set of local independences. In the next subsection, we 
 show that one can use the integrated mean and covariance to find such 
 constraints. In the subsection after next, we show that when using
 dynamical observation of the marginal distribution, we can find additional 
 constraints.


\subsection{Using Integrated Covariance}

From Equation \ref{eq:Gtilde} and the section on marginalization we see that 
every equality 
constraint in a linear structural equation model implied by the graphical 
structure imposes the same constraint
on the parameters $(\tilde{\Phi}, \Theta)$ and this allows us to find 
constraints 
in the Hawkes model. We will show by an example that these constraints may 
provide information about the underlying structure that is not contained the 
Markov equivalence class.

\begin{exmp}
	\label{exmp:eqSmallCycle}
	Consider again the example illustrated by the graph in Figure 
	\ref{fig:smallCycle}\textbf{B}. If we think of the graph as representing a 
	linear SEM, we can, e.g., use Theorem 1 and Lemma 1 of \cite{chen2014} to 
	see that the graphical structure imposes the constraint 
	
	$$\bar{C}_{14}/\bar{C}_{13} = (\bar{C}_{34} - 
	\tilde{\Phi}_{32}\bar{C}_{24})/(\bar{C}_{33} - 
	\tilde{\Phi}_{32}\bar{C}_{23}),$$ 
	
	and 
	therefore this algebraic constraint is also satisfied by the Hawkes model 
	corresponding to the graph (note that $\tilde{\Phi}_{32} = 
	\Phi_{32}/(1-\Phi_{33})$, and that the constraint 
	corresponds to
	two ways of identifying $\tilde{\Phi}_{43}$). The graphs \textbf{B} and 
	\textbf{C} in Figure 
	\ref{fig:smallCycle} (right) 
	imply the same set of local independences
	when using $\mu$-separation. However, the above constraint is satisfied in 
	\textbf{B} only and therefore this constraint allows us to 
	discriminate between the two models.
\end{exmp}

	\begin{figure*}
		%\begin{tabular}{cc}
		\begin{subfigure}{0.3\linewidth}
			\centering
			\begin{tikzpicture}[scale=0.7]
			\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em, 
					inner 
					sep = 0pt}}
			\tikzset{edge/.style = {->,> = latex', thick}}
			\tikzset{edgebi/.style = {<->,> = latex', thick}}
			\tikzset{every loop/.style={min distance=8mm, looseness=5}}
			\tikzset{vertexFac/.style = {shape=rectangle,draw,minimum 
					size=1.5em, 
					inner sep = 0pt}}
			
			% vertices
			%\draw [line width=35pt,opacity=0.1, blue,line cap=round,rounded
			%corners] (0,0.5) -- (0,2) -- (-1.5,1.5) -- (0,0.5);
			\node[vertex] (a) at  (-4,0) {$1$};
			\node[vertex] (b) at  (-2,0) {$2$};
			\node[vertex] (c) at  (0,0) {$3$};
			\node[vertex] (d) at  (2,0) {$4$};
			\node[vertex, opacity = 0] (f) at  (-1,-1.5) {};
			
			\node at (-3.5,1.25) {\Large \textbf{A}};
			
			%edges
			
			\draw[edge] (a) to (b);
			\draw[edge] (b) to (c);
			\draw[edge] (c) to (d);
			\draw[edge, bend left = 20] (d) to (a);
			
			
			\draw[edge, bend left = 35, <->] (b) to (d);			
			
			
			
			\end{tikzpicture}
		\end{subfigure}\hspace{0.05\linewidth}%
		\begin{subfigure}{0.3\linewidth}
			\centering
			\begin{tikzpicture}[scale=0.7]
			\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em, 
					inner 
					sep = 0pt}}
			\tikzset{edge/.style = {->,> = latex', thick}}
			\tikzset{edgebi/.style = {<->,> = latex', thick}}
			\tikzset{every loop/.style={min distance=8mm, looseness=5}}
			\tikzset{vertexFac/.style = {shape=rectangle,draw,minimum 
					size=1.5em, 
					inner sep = 0pt}}
			
			% vertices
			%\draw [line width=35pt,opacity=0.1, blue,line cap=round,rounded
			%corners] (0,0.5) -- (0,2) -- (-1.5,1.5) -- (0,0.5);
			\node[vertex] (a) at  (-4,0) {$1$};
			\node[vertex] (b) at  (-2,0) {$2$};
			\node[vertex] (c) at  (0,0) {$3$};
			\node[vertex] (d) at  (2,0) {$4$};
			\node[vertex, opacity = 0] (f) at  (-1,-1.5) {};
			
			\node at (-3.5,1.25) {\Large \textbf{B}};
			
			%edges
			
			\draw[edge] (a) to (b);
			\draw[edge] (b) to (c);
			\draw[edge] (c) to (d);
			\draw[edge, bend left = 20] (d) to (a);
			\draw[edge, bend right = 35] (a) to (d);			
			
			\draw[edge, bend left = 35, <->] (b) to (d);			
			
			
			
			\end{tikzpicture}
		\end{subfigure}\hspace{0.05\linewidth}%
		\begin{subfigure}{0.3\linewidth}
			\centering
			\begin{tikzpicture}[scale=0.7]
			\tikzset{vertex/.style = {shape=circle,draw,minimum size=1.5em, 
					inner 
					sep = 0pt}}
			\tikzset{edge/.style = {->,> = latex', thick}}
			\tikzset{edgebi/.style = {<->,> = latex', thick}}
			\tikzset{every loop/.style={min distance=8mm, looseness=5}}
			\tikzset{vertexFac/.style = {shape=rectangle,draw,minimum 
					size=1.5em, 
					inner sep = 0pt}}
			
			% vertices
			%\draw [line width=35pt,opacity=0.1, blue,line cap=round,rounded
			%corners] (0,0.5) -- (0,2) -- (-1.5,1.5) -- (0,0.5);
			\node[vertex] (a) at  (-4,0) {$1$};
			\node[vertex] (b) at  (-2,0) {$2$};
			\node[vertex] (c) at  (0,0) {$3$};
			\node[vertex] (d) at  (2,0) {$4$};
			\node[vertex] (f) at  (-1,-1.5) {$3^*$};
			
			\node at (-3.5,1.25) {\Large \textbf{C}};
			
			%edges
			
			\draw[edge] (a) to (b);
			\draw[edge] (b) to (c);
			\draw[edge] (c) to (d);
			\draw[edge, bend left = 20] (d) to (a);
			\draw[edge] (b) to (f);
			\draw[edge] (c) to (f);
			
			\draw[edge, bend left = 35, <->] (b) to (d);			
			
			\end{tikzpicture}
		\end{subfigure}
		%\end{tabular}
		\caption{\label{fig:bigCycle} Graphs \textbf{A} and \textbf{B} are 
			Markov equivalent, that is, entail the same local independences 
			(assuming faithfulness). They can be distinguished using an 
			equality constraint arising from using dynamical information as 
			well 
			(see Example \ref{exmp:eqBigCycle}). Graph \textbf{C} illustrates 
			the 
			use of an \emph{auxiliary variable}, see Example 
			\ref{exmp:eqBigCycle}.
			Loops (self-edges) 
			are omitted from this 
			vizualiation.}
	\end{figure*}


\subsection{Using Infinitesimal Covariance}
\label{ssec:eqInfCov}

The above subsection describes constraints that are imposed by the graphical 
structure on the integrated 
covariance. If we use other aspects of 
the distribution, 
we can obtain more constraints. In this subsection, we use Proposition 
\ref{prop:gPaId} to identify some of the entries in $\Phi$. We then treat these 
parameters as known in the integrated covariance equation which allows us to 
obtain more constraints. This is related to previous work on identification of 
causal effects in linear structural equations models with prior knowledge of 
some parameters \citep{chenIJCAI2016,chenICML2017}.

\begin{exmp}
	
	
	We consider graph \textbf{A} in Figure \ref{fig:bigCycle}. The constraint 
	from Example \ref{exmp:eqSmallCycle} does not hold in this graph. Instead 
	we assume in 
	this example that we also have access to the 
	covariance processes. In this case, the link functions 
	$\phi_{11},\phi_{14},\phi_{32}$, and $\phi_{33}$ are identified 
	(Proposition 
	\ref{prop:gPaId}), and therefore so are $\Phi_{11}, \Phi_{14} , \Phi_{32}$ 
	and 
	$\Phi_{33}$.  Leveraging this information we can again find two ways of 
	identifying 
	$\tilde{\Phi}_{43}$ and from this we will obtain an equality constraint 
	that 
	will 
	discriminate between graphs \textbf{A} and \text{B} in 
	Figure \ref{fig:bigCycle}. This can be done in different ways. We 
	construct an {\it auxiliary 
	variable} following \cite{chenIJCAI2016, chenICML2017}. This can be thought 
	of as a technique to leverage known coefficients to obtain additional 
	identification or constraints for a given model. Let $O=\{1,2,3,4\}$ and 
	let $\Phi^+$ be a 
	$5\times 5$ 
	matrix such that $\Phi_{OO}^+ = \tilde{\Phi}$, $\Phi_{52}^+ = 
	-\tilde{\Phi}_{32}, \Phi_{53}^+ 
	= 1$, and else zero, and we let $\Theta^+$ be a $5\times 5$ matrix such 
	that $\Theta_{OO}^+ = \Theta$, and $\Theta_{55}^+ = 1$ and zero else. This 
	construction corresponds to a linear SEM where we added an \emph{auxiliary 
	variable}, $3^*$, to the original model, see \ref{fig:bigCycle}\textbf{C}. 
	Then 
	the matrix $C^+$,
	
	$$
	C^+ = (I - \Phi^+)^{-1}\Theta^+(I-\Phi^+)^{-1},
	$$
	
	can be computed from $\bar{C}$ and it holds that 
	$C^+=\bar{C}_{OO}$. By direct computation, we see that $\tilde{\Phi}_{43} = 
	C_{14}^+/C_{13}^+$. To obtain a constraint, we find a second method of 
	identification of $\tilde{\Phi}_{43}$ by noting that
	
	\begin{align}
		\Theta = (I - \tilde{\Phi})\bar{C}(I-\tilde{\Phi})^T = \bar{C} - 
		\tilde{\Phi}\bar{C} - \bar{C}\tilde{\Phi}^T + 
		\tilde{\Phi}\bar{C}\tilde{\Phi}^T,
		\label{eq:expandTheta}
	\end{align}

	
	assuming that $\tilde{\Phi}$ is weakly stable. By considering the 
	$(3,4)$-entry of the above matrix equation,
	
	$$
	\tilde{\Phi}_{43} = (\bar{C}_{34} - 
	\tilde{\Phi}_{32}\bar{C}_{24})/(\bar{C}_{33} - 
	\tilde{\Phi}_{32}\bar{C}_{23}).
	$$
	
 Again, we obtained two ways of identifying 
 the parameter $\tilde{\Phi}_{43}$, that is, a constraint
 
 $$
 C_{14}^+/C_{13}^+ = (\bar{C}_{34} - 
 \tilde{\Phi}_{32}\bar{C}_{24})/(\bar{C}_{33} - 
 \tilde{\Phi}_{32}\bar{C}_{23})
 $$
 
 using that $\tilde{\Phi}_{32} = {\Phi}_{32}/(1-\Phi_{33})$ is known as 
 ${\Phi}_{32}$ and 
 ${\Phi}_{33}$ are known. This constraint is satisfied in 
 Figure
 \ref{fig:bigCycle}\textbf{A}, 
 but not in \ref{fig:bigCycle}\textbf{B}, and it allows us to 
 distinguish between 
 the two.
 
 \label{exmp:eqBigCycle}
\end{exmp}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}

We obtained identification results and equality constraints for Hawkes process 
models through the similarity between integrated Hawkes covariance and observed 
covariance of a linear structural equation model. We also argued that more 
constraints can be found by considering infinitesimal covariance. These 
equality constraints will be useful for structure learning as they can 
supplement constraint-based learning based on tests of local independence.

A Hawkes process is defined using the set of $\phi_{\beta\alpha}$-functions 
along 
with the $\mu_\alpha$-constants for $\alpha,\beta$ in the finite set $V$ and a 
Hawkes process model is in that sense nonparametric.  Constraints that arise 
from the integrated covariance offer also a dimension reduction in that we 
actually test these constraints using a finite parameter space instead of a 
function space. This should make them more suitable to employ in data analysis 
as estimation of the link functions, $\{\phi_{\beta\alpha} \}$, is a 
challenging problem even in the case of full 
observation and partial observation only adds to the 
complexity of this task.



%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{contributions} % will be removed in pdf for initial submission,
                      % so you can already fill it to test with the
                      % ‘accepted’ class option

\end{contributions}

\begin{acknowledgements} % will be removed in pdf for initial submission,
                         % so you can already fill it to test with the
                         % ‘accepted’ class option
    This work was supported by a research grant from
    VILLUM FONDEN (13358).
\end{acknowledgements}

\appendix

\section{Marginalization}
\label{sec:marg}

In this section and for a matrix $A$, we define $A_{\bar{I}\bar{J}}^{-1} = 
(A^{-1})_{\bar{I}\bar{J}}$ and 
$A_{\bar{I}\bar{J}}^{-T}=(A^{-T})_{\bar{I}\bar{J}}$. Consider an invertible 
block matrix 

$$
M=
\begin{pmatrix}
A & B \\
C & D
\end{pmatrix}.
$$

We let $M / D = A - BD^{-1}C$ denote the \emph{Schur complement} of block D in 
the matrix M. It holds that

$$
M^{-1}=
\begin{pmatrix}
(M/D)^{-1} & -(M/D)^{-1}BD^{-1} \\
-D^{-1}C(M/D)^{-1} & D^{-1}(I + C(M/D)^{-1}BD^{-1})
\end{pmatrix}.
$$

Therefore, $$(I - {\Phi})_{OO}^{-1} = ((I - {\Phi})/(I - 
{\Phi})_{UU})^{-1} = (I- \bar{\Phi})^{-1}$$ by definition of 
$\bar{\Phi}$. 
Furthermore, 

$$
(I-{\Phi})_{OU}^{-1} = 
-(I-\bar{\Phi})^{-1}(I-{\Phi})_{OU}(I-{\Phi})((I-{\Phi})_{UU})^{-1}
$$

Let $I = I_n$ and $A_{\bar{I}}=A_{\bar{I}\bar{I}}$. From the above, it 
follows 
that

\begin{align*}
\Theta & = (I_k - \bar{\Phi})[(I_n - 
{\Phi})_{O}^{-1}{\Lambda}_{O}(I - {\Phi})_{O}^{-T} + \\ 
&  
(I 
- 
{\Phi}_{OU}^{-1}){\Lambda}_{U}(I - 
{\Phi})_{UO}^{-T}](I_k - 
\bar{\Phi})^T \\
& = {\Lambda}_{O} + (I_k - \bar{\Phi})(I 
- 
{\Phi})_{OU}^{-1}{\Lambda}_{U}((I - 
{\Phi})_{UO}^{-T}(I_k - 
\bar{\Phi})^T \\
& = {\Lambda}_{O} + \\ & (I - {\Phi})_{OU}(I - 
{\Phi})_{U}^{-1}{\Lambda}_{U}((I - 
{\Phi})_{U}^{-T}(I - 
{\Phi})_{U}^{-T}(I - 
{\Phi})_{OU}^T \\
& = {\Lambda}_{O} + {\Phi}_{OU}(I - 
{\Phi})_{U}^{-1}{\Lambda}_{U}((I - 
{\Phi})^{-T})_{U}(I - 
{\Phi})_{U}^{-T} 
{\Phi}_{OU}^T \\
& = {\Lambda}_{O} + {\Phi}_{OU}(I - 
{\Phi})_{U}^{-1}{\Lambda}_{U} (I - 
{\Phi})_{U}^{-T}
{\Phi}_{OU}^T.
\end{align*}


\bibliography{parentalLearning,C:/Users/swmo/Desktop/-/UCPH/PhD/learning/parental/causalScreening/parentalLearning}
\end{document}
